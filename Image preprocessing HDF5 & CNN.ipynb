{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "error_table = \"YTFErrors.csv\"\n",
    "error_list = []\n",
    "with open(error_table) as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)  \n",
    "    error_header = next(csv_reader)  \n",
    "    for row in csv_reader:  \n",
    "        error_list.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "temp = []\n",
    "for error in error_list:\n",
    "    temp.append(re.split(r'\\W+', error)[0])\n",
    "error_list = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8820446ba426402385728a8441aa7bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "name_list = []\n",
    "for root, dirs, files in tqdm_notebook(os.walk(\"frame_images_DB\")):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            name = root.split('/')[1]\n",
    "            if name not in error_list:\n",
    "                name_list.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Katharine_Hepburn\" in error_list)\n",
    "print(\"Katharine_Hepburn\" in name_list)\n",
    "\n",
    "# tested OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_test_split & Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08466874f7d94f089ea28a241c9225e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# list first folders of each person as the training data\n",
    "temp_list = list(set(name_list))\n",
    "train_set = set()\n",
    "for root, dirs, files in tqdm_notebook(os.walk(\"frame_images_DB\")):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            name = root.split('/')[1]            \n",
    "            if name in temp_list:\n",
    "                train_set.add(root)\n",
    "                temp_list.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cb1e58b26f442b8b5ddf7aaed3e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# list second folders of applicable person as the testing data\n",
    "temp_list = list(set(name_list))\n",
    "test_set = set()\n",
    "for root, dirs, files in tqdm_notebook(os.walk(\"frame_images_DB\")):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            name = root.split('/')[1]            \n",
    "            if (name in temp_list) & (root not in train_set):\n",
    "                test_set.add(root)\n",
    "                temp_list.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train_set Randomly select frames, 48 per person\n",
    "import random\n",
    "train_addrs = []\n",
    "for path in train_set:\n",
    "    file = random.sample(os.listdir(path),48)\n",
    "    for i in file:\n",
    "        train_addrs.append(path+'/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test_set Randomly select frames, 10 per person\n",
    "import random\n",
    "test_addrs = []\n",
    "for path in test_set:\n",
    "    file = random.sample(os.listdir(path),10)\n",
    "    for i in file:\n",
    "        test_addrs.append(path+'/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9510"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_addrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shape\n",
    "train_shape = (len(train_addrs), 24, 24, 3)\n",
    "test_shape = (len(test_addrs), 24, 24, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels\n",
    "train_labels = [i.split('/')[1] for i in train_addrs]\n",
    "test_labels = [i.split('/')[1] for i in test_addrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CascadeClassifier 0x13b6efed0>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "#create a CascadeClassifier Object\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(cascadePath)\n",
    "#checking to see if classifier has been loaded properly\n",
    "print(face_cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"frame_images_DB/Aaron_Eckhart/0/0.555.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1698: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-61886cff4f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mminSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1698: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(test)\n",
    "gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 480, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1698: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-188ada099bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#         minSize=(30, 30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1698: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for file in train_addrs:\n",
    "    \n",
    "    image=cv2.imread(file)\n",
    "    #reading the image as gray scale\n",
    "    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces in the image\n",
    "    #search for the face co-ordinates in the image\n",
    "    #detect multiscale is the method to search for the face rectangle co-ordinates \n",
    "    #scale factor =1.1 decreases the shape value by 5% until the face is found. The smaller this value, the more accuracy\n",
    "    #we may want to hyper parameter tune with scaleFactor and neighbors\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5\n",
    "#         minSize=(30, 30)\n",
    "    )\n",
    "    #print(\"Found {0} faces!\".format(len(faces)))\n",
    "    \n",
    "    #Crop padding\n",
    "    #may not be needed\n",
    "    left = 10\n",
    "    right = 10\n",
    "    top = 10\n",
    "    bottom = 10\n",
    "    \n",
    "    #draw a rectange around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        print(x, y, w, h)\n",
    "        #method to create the face rectangle\n",
    "        #cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 23)\n",
    "    \n",
    "    #crop the image\n",
    "    image  = image[y-top:y+h+bottom, x-left:x+w+right]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"test_img\": shape (15312, 24, 24, 3), type \"|i1\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize hdf5\n",
    "import numpy as np\n",
    "import h5py\n",
    "hdf5_path = 'dataset.hdf5'\n",
    "hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "hdf5_file.create_dataset(\"train_img\", train_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"test_img\", test_shape, np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.create_dataset(\"train_mean\", train_shape[1:], np.float32)\n",
    "dt = h5py.special_dtype(vlen=str) \n",
    "hdf5_file.create_dataset(\"train_labels\", (len(train_addrs),), dt)\n",
    "hdf5_file[\"train_labels\"][...] = train_labels\n",
    "hdf5_file.create_dataset(\"test_labels\", (len(test_addrs),), dt)\n",
    "hdf5_file[\"test_labels\"][...] = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bd2ee4f9294fb8b55d8ce49b7fc32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61248), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6d68f0e13993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# save the image and calculate the mean so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhdf5_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;34m\"\"\" Open an object in the file \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# a numpy array to save the mean of the images\n",
    "mean = np.zeros(train_shape[1:], np.float32)\n",
    "# loop over train addresses\n",
    "for i in tqdm_notebook(range(len(train_addrs))):\n",
    "    # read an image and resize to (24, 24)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = train_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (24, 24), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "\n",
    "    # save the image and calculate the mean so far\n",
    "    hdf5_file[\"train_img\"][i, ...] = img[None]\n",
    "    mean += img / float(len(train_labels))\n",
    "\n",
    "# loop over test addresses\n",
    "for i in tqdm_notebook(range(len(test_addrs))):\n",
    "    # read an image and resize to (24, 24)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = test_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (24, 24), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "\n",
    "    # save the image\n",
    "    hdf5_file[\"test_img\"][i, ...] = img[None]\n",
    "# save the mean and close the hdf5 file\n",
    "hdf5_file[\"train_mean\"][...] = mean\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "hdf5_path = 'dataset.hdf5'\n",
    "subtract_mean = False\n",
    "# open the hdf5 file\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file[\"train_mean\"][0, ...]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "# Total number of samples\n",
    "data_num = hdf5_file[\"train_img\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAADb0lEQVR4nK2Uf4gUZRjHH1/fXl+GcXgdpmFdpr3lutuW5ViOcxURCcvN4vKCusDTo1Ix748sEZQzLAMJMrsgkrQfFISdXCZ2KnngaSRKXSLXIccRsq7XtizbMq7TMAzj29t7/XGbpjftRfj+9Tzvl/fzfr/PC++cqakpuBcL3RPK7KD585t7X3/7v4Dm1In2UkdnvEnVY3EFs7Uvb/ufjjasfDqdQClaWZ2wlrfp/e8erA/Cf8Cf98HcmYLgeWErnkJztqtVtfbW5CyOLvz4fagguJKrivG8D9xWmeSyNIujRsMKByFJA1+Vbn7k0sCVI00GWW7DorXd/+posjgWKrRSJqjd1maVK/a6bMZKZBYtTddxhDCOhgqF6oQrcLlSeHRZerIw0fnM6pOHT9QDMRr+cKn0Ksq1i1f8wuR4qQr73z/QsWtXPdD4+M+hwtZDn7qIIKKXKgFEoyvau+pQAACrZuRW89uv19Zv3GKjAIMXoRh7DgdOXakT98zRYzs/HvRADA58sWDhwhBHVozdaro2rxsa/oZVCEY6lWpcMV0PBKJGxDQb4xozzp07u3nLa+HRGLsN0hQGAMM/DVGMGbGoBlHTzJX4hZH8aNHe17cbACQqhoPw37N+49W3iFZrkprKcNEgJBnRVUPRowyI8UDzQwCgK8YjT3aGgAhWp6uLY6NC1twV8zKlcKQGiE6mGI+zSHs2WxuqpPo/QtwGqUyfrgxVcmx/3rsDANJLV6UyCTPZoslYVDcV5li4lsgBTpGYCap9Iz3dPZz4JsLPPhwzT+cb+g+d7/+oMbBtu+r4ZTXSaMWWfFtJLqvm+i4ddigKOPmq/5M7HNXuwXLv7p1PtSDDcxtWJACg78MxwzJjiXjrkmwiqhs6GTzyWUNLZs/zmxDHWFPvjgZw85er157IJC+ferMp0cosC6gKAMfPH8xhHdEWzqhD44Ijg8Th8fsXqKUDr7THOP566Ie7QPP69r0nvbLiSM8ruKJ8XQtuvvgOAOzZ+x0gV3BCwBci+OB4DwCAqxIptm/Mnjo6MCMaIkrge4KXi1XkAPdhnq4BwJfD+09MICJARdrIqF87IangCoiAKPIO0IPNi4E4GiogKami+V5F2AHo5Wn5uW0dRdLZ1HV65Y41tRMdj0lZ9rx8W9LqfqH39+s3prf/Aos0TMb8IO07AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=24x24 at 0x11FD30518>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import array_to_img\n",
    "img_test = array_to_img(hdf5_file[\"train_img\"][0])\n",
    "img_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61248"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 22, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1595)              205755    \n",
      "=================================================================\n",
      "Total params: 1,044,475\n",
      "Trainable params: 1,044,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      " - 55s - loss: 7.7315 - acc: 0.0011\n",
      "Epoch 2/3\n",
      " - 54s - loss: 7.3479 - acc: 0.0038\n",
      "Epoch 3/3\n",
      " - 56s - loss: 7.1110 - acc: 0.0160\n"
     ]
    }
   ],
   "source": [
    "# convert labels to OHE\n",
    "from keras import utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(hdf5_file[\"train_labels\"])\n",
    "encoded_train = encoder.transform(hdf5_file[\"train_labels\"])\n",
    "encoded_test = encoder.transform(hdf5_file[\"test_labels\"])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_train = utils.to_categorical(encoded_train)\n",
    "dummy_test = utils.to_categorical(encoded_test)\n",
    "\n",
    "\n",
    "# Build NN Structure\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "batch_size = 1280\n",
    "num_classes = dummy_train.shape[1]\n",
    "epochs = 3\n",
    "input_shape = (24, 24, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation ='relu',\n",
    "                 input_shape =input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history1 = model.fit(hdf5_file[\"train_img\"], dummy_train,\n",
    "                     batch_size = batch_size,\n",
    "                     epochs = epochs,\n",
    "                     verbose = 2,\n",
    "                     shuffle = 'batch'\n",
    "                     )\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
